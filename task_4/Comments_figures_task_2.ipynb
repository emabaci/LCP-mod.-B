{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76eb5730",
   "metadata": {},
   "source": [
    "## Kommentarer til kode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab7786",
   "metadata": {},
   "source": [
    "(Under headline)\n",
    "Comparison of a convolutional neural network and a XGBoost on the same problem with classification of timeseries. The convolution network has been given the original generated data, while the XGBoost has been trained with the extracted features using tsfresh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8364fcc",
   "metadata": {},
   "source": [
    "(Under subtitle: \"Creation of models\")The CNN-model used in the task is the same as used in exercise 3 from last week. A convolution network with Lasso-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238f11a",
   "metadata": {},
   "source": [
    "Task 2\n",
    "Goal; Find the simplest yet effective XGBoost model that keeps a good accuracy. \n",
    "\n",
    "(Under list of ten best models)\n",
    "The Gridsearch shows that the best XGB-models that score the best are those with the highest number of trees and max_depth among the values tested. This indicates that the models in general performs better with an increased complexity. However, good results can be obtained with a lower amount of decision trees and smaller depth. The top ten models seem to indicate that the depth and number of estimators are the most important factors as the top ten models score approximately the same, even though the values of some of the other parameters differ.\n",
    "\n",
    "(After heatmaps)\n",
    "The heatplots shows the improvement in accuracy as a function of the different parameters. In general, with an increasing depth one can see that good accuracies can also be obtained with a lower number of estimators. Below a depth of 6 however, the accuracy drops substantially, and an increased number of estimators is not able to wigh up for that. This implies that the depth of the model is perhaps the most important factor, while the number of estimators can increase the accuracy further. This does hoewever yield a more complex model. One can see that a model with a depth of 6 and number of estimators equal to 7 yields good results, and could therefore be a good compromize between the two parameters. This is investigated further down.\n",
    "\n",
    "The heatplot for lambda and gamma shows that the optimal values for lambda and gamma are 1 and zero respectively. This is also visible in the list of the top ten models, as these are the values they use.\n",
    "\n",
    "The best learning rate for the classification of the time series seem to be 0.3, and the accuracy increases further with the number of estimators as expected. The parameter max delta step however seem to have little impact on the result. In the overview of the best models, the models with all other parameters equal perform the same regardless of the value of max delta step. One can also see this from the heatmap as all the values tested generate about the same accuracy with the same number of estimators.\n",
    "\n",
    "(Before or after list of models with deth 7 and trees 6)\n",
    "For comparison with the best models of the grid search, the ten best estimators with a depth of 7 and number of estimators equal to 6 are shown, as this less complex combination was seen to yield good results in the investigation of estimators and depth. Except from the fact that the learning rate is increased to 0.4 in this case, the rest of the parameters show the same trend as indicated by the different heatmaps. Even though these models are significantly les complexs than the best performing models, their accuracies are still good, and not far from the performancees of the best models. Hence, this implies that this model is suffient for the task, and that the depth, number of estimators, and learning rate are the most important parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a4b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
